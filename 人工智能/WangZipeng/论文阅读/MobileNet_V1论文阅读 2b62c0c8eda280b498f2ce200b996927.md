# MobileNet_V1论文阅读

MobileNetV1（2017，Google）是一种专为移动和嵌入式设备设计的轻量级卷积神经网络。其核心是深度可分离卷积——将标准卷积分解为深度卷积（逐通道空间滤波）和1×1 逐点卷积（通道组合），在大幅降低计算量（约 8–9 倍）和模型大小的同时，保持较高 accuracy。通过宽度乘子（α）和分辨率乘子（ρ），可灵活平衡精度、速度与资源消耗，广泛应用于手机端视觉任务。

[MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications（点击跳转）](/人工智能/WangZipeng/论文阅读/PDF/Howard%20等%20-%202017%20-%20MobileNets%20Efficient%20Convolutional%20Neural%20Networks%20for%20Mobile%20Vision%20Applications.compare.pdf)

## 前置知识点 ：

[神经网络学习](/人工智能/WangZipeng/前置知识点/经典神经网络.md)
[卷积网路](/人工智能/WangZipeng/前置知识点/卷积神经网络.md)

### **特点**：提出可分离卷积网络层

Depthwise Separable Convolution

### ***具体原理： [1. 深度卷积](#1-深度卷积) + [2. 1X1卷积核](#2-1x1卷积核---逐点卷积)***



## 1. 深度卷积

***假设输入：***

图像/特征图：`H=5, W=5, C_in=3`（比如 RGB 图）

使用 **3×3 深度卷积**

***1）步骤 1：为每个输入通道分配独立的卷积核***
通道 0（Red）→ 使用 3×3 卷积核 K₀
通道 1（Green）→ 使用 3×3 卷积核 K₁
通道 2（Blue）→ 使用 3×3 卷积核 K₂
总共仅需 3 个卷积核（而非标准卷积的 C_out × C_in 个）

***2）步骤 2：对每个通道单独进行 2D 卷积***
K₀ 卷积 Red 通道 → 输出通道 0
K₁ 卷积 Green 通道 → 输出通道 1
K₂ 卷积 Blue 通道 → 输出通道 2

***3）步骤 3：拼接输出***
输出形状：H' × W' × C_in（例如 padding=same 时为 5×5×3）

![截屏2025-11-25 10.46.42.png](/人工智能/WangZipeng/images/MobileNet_V1论文阅读/截屏2025-11-25_10.46.42.png)

**M表示输入特征图的有几层**

*如R、B、G三层 （经典图层）*

**Dk表示卷积核的权重的大小**

**1表示是对于特征图单层的卷积核**

## 2. 1X1卷积核 - 逐点卷积

如在 `[1920，1080，3]` 的图片输入数据当中进行逐次卷积——在`[1920,1080]` 的平面至上滑动卷积（步长为1）

![截屏2025-11-25 10.19.00.png](/人工智能/WangZipeng/images/MobileNet_V1论文阅读/截屏2025-11-25_10.19.00.png)

**例子**输入（3×3，3 通道）：

1×1 卷积（输出 2 通道）：

- 权重核1: [w1_r, w1_g, w1_b]
- 权重核2: [w2_r, w2_g, w2_b]

输出 (0,0) 位置：

- 通道0: `w1_r * r00 + w1_g * g00 + w1_b * b00`
- 通道1: `w2_r * r00 + w2_g * g00 + w2_b * b00`

然后滑动到 (0,1)，用**同样的权重**计算：

```
位置 (0,0): [r00, g00, b00]
位置 (0,1): [r01, g01, b01]
...
位置 (2,2): [r22, g22, b22]
```

![截屏2025-11-25 11.43.18.png](/人工智能/WangZipeng/images/MobileNet_V1论文阅读/截屏2025-11-25_11.43.18.png)

## 3. 普通卷积层 与 分离卷积网络层的区别

**标准卷积的计算成本为:  DK · DK · M · N · DF · DF**

**深度可分离卷积的计算成本:  DK · DK · M · DF · DF + M · N · DF · DF**

![截屏2025-11-25 10.53.26.png](/人工智能/WangZipeng/images/MobileNet_V1论文阅读/截屏2025-11-25_10.53.26.png)

## 4. 宽度乘子α :更薄的模型

Width Multiplier: Thinner Models

![截屏2025-11-25 14.19.54.png](/人工智能/WangZipeng/images/MobileNet_V1论文阅读/截屏2025-11-25_14.19.54.png)

宽度乘子 α 的作用是在每一层上均匀地缩减网络

**α = 1 是基准MobileNet,而 α < 1 是精简版 MobileNet**

通过调整α系数来设置网络规模参数

## 5. 分辨率乘子:减少表示

Resolution Multiplier: Reduced Representation

![截屏2025-11-25 14.20.15.png](/人工智能/WangZipeng/images/MobileNet_V1论文阅读/截屏2025-11-25_14.20.15.png)

减少神经网络计算成本的第二个超参数是分辨率乘子 ρ

**应用于输入图像,并且每一层的内部表示随后都 通过相同的乘数进行缩减**